{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b77c6a-341a-40da-be33-5e55481d0460",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e003f2-369a-4415-8cd7-03c0b567c1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e6766-100e-4264-a85f-90a2b3ad378d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85301aac-1789-4074-95ea-6c7e5f9d822e",
   "metadata": {},
   "source": [
    "Most of the tokenizer use a subword tokenization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "656c23c0-a8a5-4716-8ad8-a5772f838910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'right', 'at', 'the', 'point', 'where', 'i', \"'\", 'm', 'loss', '##ing', 'everything']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"I'm right at the point where I'm lossing everything\"\n",
    "tokens = tokenizer.tokenize(raw_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320d12c-d0db-4ee2-a634-be32a4871ce4",
   "metadata": {},
   "source": [
    "## Map the tokens to their respective id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73fd5542-5532-4b2b-b0ad-9e1d5461b19b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 1049, 2157, 2012, 1996, 2391, 2073, 1045, 1005, 1049, 3279, 2075, 2673]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084929f6-e5f5-4af0-b8ba-d82b20b376e6",
   "metadata": {},
   "source": [
    "## Add the special characters\n",
    "For the token to be prepared for the model, we need it to add the special characters.\n",
    "\n",
    "101 -> Beginning of the text\n",
    "\n",
    "102 -> End of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c9e89da-791b-429c-9e2b-cae74fec1e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 1049, 2157, 2012, 1996, 2391, 2073, 1045, 1005, 1049, 3279, 2075, 2673, 102]\n"
     ]
    }
   ],
   "source": [
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a2886-46f3-462d-85d2-0245fa3d3b35",
   "metadata": {},
   "source": [
    "Let's print token with special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e65b7d9-fc5d-4cfe-9130-370aca4e802b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i'm right at the point where i'm lossing everything [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_text)\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fb4b3-83c5-4f87-8ea8-029fb07e03b0",
   "metadata": {},
   "source": [
    "## The important part\n",
    "We've seen how the tokenizer works, but we only have to call the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ae2ce7e-43e7-4bd5-b500-77bd438f682d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2157, 2012, 1996, 2391, 2073, 1045, 1005, 1049, 3279, 2075, 2673, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_text)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cdf09-20bc-447d-b897-a38ffcbbcd36",
   "metadata": {},
   "source": [
    "# Batch Inputs\n",
    "In general the sentences we want to pass through our model won't all have the same lengths. \n",
    "\n",
    "Let's see an example with sentyment analysys pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3c73280b-23b3-4b15-acc9-30e6e8d61b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023, 2003, 2054, 1045, 1005, 2042, 8074, 2026, 4920, 2166]\n",
      "[1045, 2738, 2031, 1037, 3760, 16183, 28221, 2021, 2562, 4083, 1037, 2843]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentences = [\n",
    "\"This is what I'been expecting my hole life\",\n",
    "\"I rather have a smaller sallary but keep learning a lot\"    \n",
    "]\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "for id in ids:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cba30c-9d91-4709-a78f-809dc48c24a2",
   "metadata": {},
   "source": [
    "We get to lists of different lengths.\n",
    "\n",
    "Trying to create a tensor or a NumPy array from those two lists will result in an error, because all array and tensors should be rectangular. One way to overcome this limitation, is to make the second sentence the same length as the first, by adding a special token as many times as necessary. \n",
    "\n",
    "The other option would be to truncate the first sentence to be as short as the second one, but by doing we may loose some information inmportant to classify the sentence.\n",
    "\n",
    "In general we only truncate sentences when they are longer than the maximum length the model can handle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2db9a-5df9-4052-ad1e-27867267ed89",
   "metadata": {},
   "source": [
    "The value used to pad the second sentence should not be picked randomly. The model has been pretrained with a certain padding ID-> tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14d0393a-8797-4e00-9e88-d51b5537ecab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 2054, 1045, 1005, 2042, 8074, 2026, 4920, 2166]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same length sentence\n",
    "import copy\n",
    "\n",
    "ids_same_size = copy.deepcopy(ids)\n",
    "\n",
    "max_len = max(len(id) for id in ids_same_size) \n",
    "\n",
    "for index,id in enumerate(ids_same_size):\n",
    "    while len(id) < max_len:\n",
    "        ids_same_size[index].append(tokenizer.pad_token_id)\n",
    "\n",
    "ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6859b-5019-4365-8f2e-a32bf46e0683",
   "metadata": {},
   "source": [
    "Now that we padded the sentences, we can make a batch with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97710f7e-a050-4814-abed-4c8dfae1637c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 2054, 1045, 1005, 2042, 8074, 2026, 4920, 2166]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "ids1 = tf.constant(\n",
    "    [ids[0]]\n",
    ")\n",
    "\n",
    "ids2 = tf.constant([ids[1]])\n",
    "\n",
    "all_ids = tf.constant(\n",
    "    [ids_same_size[0],ids_same_size[1]]\n",
    ")\n",
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1e09f09-09a7-4883-8667-042e79079331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 2.6742957 -2.1284556]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 1.4748219 -1.3111227]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 2.8111033 -2.2949333]\n",
      " [ 1.474822  -1.3111233]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "print(model(ids1).logits)\n",
    "print(model(ids2).logits)\n",
    "print(model(all_ids).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b06501-d444-4077-8ba9-bc9a5bad105c",
   "metadata": {},
   "source": [
    "We get different results for id[0]. This is because transformers models make heavy use of attention layers, looking all of the words in a sentence. In ids_same_size[0] is also counting de [PAD] values. \n",
    "\n",
    "To get the same results with or without padding, we need to indicate to the attention layers that they should ignore those padding tokens.\n",
    "\n",
    "This is done by creating an attention mask. This is a tensor with the same inputs as the input IDs, with values 1 (to pay attention) and 0 (to pay no attention).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9a815db-cee7-4aeb-85e1-f7f5eaadb819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_mask_0=[]\n",
    "attention_mask_1 =[]\n",
    "attention_mask = [attention_mask_0,attention_mask_1]\n",
    "\n",
    "for index,id_same_size in enumerate(ids_same_size):\n",
    "    for element in id_same_size:\n",
    "        if (element != tokenizer.pad_token_id):\n",
    "            attention_mask[index].append(1)\n",
    "        else:\n",
    "            attention_mask[index].append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08fb169f-b5be-4df4-9625-5486438596f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_mask = tf.constant(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c09bfcfb-2339-4018-915f-b05ded48f976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c1dee33-8791-4144-aa7e-4c279cd56613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 2.6742957 -2.1284556]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 1.4748219 -1.3111227]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output1=model(ids1)\n",
    "print(output1.logits)\n",
    "output2 = model(ids2)\n",
    "print(output2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "163c7384-5635-4aa3-a4e0-4bee877c2e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2.6742976 -2.1284575]\n",
      " [ 1.474822  -1.3111233]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output_all = model(all_ids,attention_mask=attention_mask)\n",
    "print(output_all.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8585-7fdc-4a2c-a33d-088dba870d3b",
   "metadata": {},
   "source": [
    "Same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b4c7e-1679-484e-893a-30ac207aa3fc",
   "metadata": {},
   "source": [
    "THIS IS ALL DONE BY THE TOKENIZER WHEN WHRN YOU APPLY IT TO SEVERAL SENTENCES WITH THE FLAG padding=True.\n",
    "\n",
    "IT WILL APPLY THE PADDING TO THE PROPER VALUE FOR THE SMALLER SENTENCES AND CREATE THE APPROPIATE ATTENTION MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb677b0-e342-4a8f-9a94-20f4c4a36a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
