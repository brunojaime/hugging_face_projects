{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b77c6a-341a-40da-be33-5e55481d0460",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e003f2-369a-4415-8cd7-03c0b567c1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e6766-100e-4264-a85f-90a2b3ad378d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85301aac-1789-4074-95ea-6c7e5f9d822e",
   "metadata": {},
   "source": [
    "Most of the tokenizer use a subword tokenization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656c23c0-a8a5-4716-8ad8-a5772f838910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'right', 'at', 'the', 'point', 'where', 'i', \"'\", 'm', 'loss', '##ing', 'everything']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"I'm right at the point where I'm lossing everything\"\n",
    "tokens = tokenizer.tokenize(raw_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320d12c-d0db-4ee2-a634-be32a4871ce4",
   "metadata": {},
   "source": [
    "## Map the tokens to their respective id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73fd5542-5532-4b2b-b0ad-9e1d5461b19b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 1049, 2157, 2012, 1996, 2391, 2073, 1045, 1005, 1049, 3279, 2075, 2673]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084929f6-e5f5-4af0-b8ba-d82b20b376e6",
   "metadata": {},
   "source": [
    "## Add the special characters\n",
    "For the token to be prepared for the model, we need it to add the special characters.\n",
    "\n",
    "101 -> Beginning of the text\n",
    "\n",
    "102 -> End of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c9e89da-791b-429c-9e2b-cae74fec1e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 1049, 2157, 2012, 1996, 2391, 2073, 1045, 1005, 1049, 3279, 2075, 2673, 102]\n"
     ]
    }
   ],
   "source": [
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a2886-46f3-462d-85d2-0245fa3d3b35",
   "metadata": {},
   "source": [
    "Let's print token with special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e65b7d9-fc5d-4cfe-9130-370aca4e802b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i'm right at the point where i'm lossing everything [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_text)\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fb4b3-83c5-4f87-8ea8-029fb07e03b0",
   "metadata": {},
   "source": [
    "## The important part\n",
    "We've seen how the tokenizer works, but we only have to call the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae2ce7e-43e7-4bd5-b500-77bd438f682d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2157, 2012, 1996, 2391, 2073, 1045, 1005, 1049, 3279, 2075, 2673, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_text)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cdf09-20bc-447d-b897-a38ffcbbcd36",
   "metadata": {},
   "source": [
    "# Batch Tokenizer\n",
    "In general the sentences we want to pass through our model won't all have the same lengths. \n",
    "\n",
    "Let's see an example with sentyment analysys pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c73280b-23b3-4b15-acc9-30e6e8d61b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023, 2003, 2054, 1045, 1005, 2042, 8074, 2026, 4920, 2166]\n",
      "[1045, 2738, 2031, 1037, 3760, 16183, 28221, 2021, 2562, 4083, 1037, 2843]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentences = [\n",
    "\"This is what I'been expecting my hole life\",\n",
    "\"I rather have a smaller sallary but keep learning a lot\"    \n",
    "]\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "for id in ids:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cba30c-9d91-4709-a78f-809dc48c24a2",
   "metadata": {},
   "source": [
    "We get to lists of different lengths.\n",
    "\n",
    "Trying to create a tensor or a NumPy array from those two lists will result in an error, because all array and tensors should be rectangular. One way to overcome this limitation, is to make the second sentence the same length as the first, by adding a special token as many times as necessary. \n",
    "\n",
    "The other option would be to truncate the first sentence to be as short as the second one, but by doing we may loose some information inmportant to classify the sentence.\n",
    "\n",
    "In general we only truncate sentences when they are longer than the maximum length the model can handle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2db9a-5df9-4052-ad1e-27867267ed89",
   "metadata": {},
   "source": [
    "The value used to pad the second sentence should not be picked randomly. The model has been pretrained with a certain padding ID-> tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14d0393a-8797-4e00-9e88-d51b5537ecab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 2054, 1045, 1005, 2042, 8074, 2026, 4920, 2166]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same length sentence\n",
    "import copy\n",
    "\n",
    "ids_same_size = copy.deepcopy(ids)\n",
    "\n",
    "max_len = max(len(id) for id in ids_same_size) \n",
    "\n",
    "for index,id in enumerate(ids_same_size):\n",
    "    while len(id) < max_len:\n",
    "        ids_same_size[index].append(tokenizer.pad_token_id)\n",
    "\n",
    "ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6859b-5019-4365-8f2e-a32bf46e0683",
   "metadata": {},
   "source": [
    "Now that we padded the sentences, we can make a batch with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97710f7e-a050-4814-abed-4c8dfae1637c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2023, 2003, 2054, 1045, 1005, 2042, 8074, 2026, 4920, 2166]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "ids1 = tf.constant(\n",
    "    [ids[0]]\n",
    ")\n",
    "\n",
    "ids2 = tf.constant([ids[1]])\n",
    "\n",
    "all_ids = tf.constant(\n",
    "    [ids_same_size[0],ids_same_size[1]]\n",
    ")\n",
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e09f09-09a7-4883-8667-042e79079331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 2.6742945 -2.1284542]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 1.4748228 -1.3111241]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 2.8111045 -2.2949347]\n",
      " [ 1.474823  -1.3111236]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "import tf_keras as keras\n",
    "\n",
    "print(model(ids1).logits)\n",
    "print(model(ids2).logits)\n",
    "print(model(all_ids).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b06501-d444-4077-8ba9-bc9a5bad105c",
   "metadata": {},
   "source": [
    "We get different results for id[0]. This is because transformers models make heavy use of attention layers, looking all of the words in a sentence. In ids_same_size[0] is also counting de [PAD] values. \n",
    "\n",
    "To get the same results with or without padding, we need to indicate to the attention layers that they should ignore those padding tokens.\n",
    "\n",
    "This is done by creating an attention mask. This is a tensor with the same inputs as the input IDs, with values 1 (to pay attention) and 0 (to pay no attention).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9a815db-cee7-4aeb-85e1-f7f5eaadb819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_mask_0=[]\n",
    "attention_mask_1 =[]\n",
    "attention_mask = [attention_mask_0,attention_mask_1]\n",
    "\n",
    "for index,id_same_size in enumerate(ids_same_size):\n",
    "    for element in id_same_size:\n",
    "        if (element != tokenizer.pad_token_id):\n",
    "            attention_mask[index].append(1)\n",
    "        else:\n",
    "            attention_mask[index].append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08fb169f-b5be-4df4-9625-5486438596f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_mask = tf.constant(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c1dee33-8791-4144-aa7e-4c279cd56613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 2.6742945 -2.1284542]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 1.4748228 -1.3111241]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output1=model(ids1)\n",
    "print(output1.logits)\n",
    "output2 = model(ids2)\n",
    "print(output2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "163c7384-5635-4aa3-a4e0-4bee877c2e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2.674298  -2.128457 ]\n",
      " [ 1.474823  -1.3111236]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output_all = model(all_ids,attention_mask=attention_mask)\n",
    "print(output_all.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8585-7fdc-4a2c-a33d-088dba870d3b",
   "metadata": {},
   "source": [
    "Same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b4c7e-1679-484e-893a-30ac207aa3fc",
   "metadata": {},
   "source": [
    "THIS IS ALL DONE BY THE TOKENIZER WHEN WHRN YOU APPLY IT TO SEVERAL SENTENCES WITH THE FLAG padding=True.\n",
    "\n",
    "IT WILL APPLY THE PADDING TO THE PROPER VALUE FOR THE SMALLER SENTENCES AND CREATE THE APPROPIATE ATTENTION MASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c750afb",
   "metadata": {},
   "source": [
    "# Preprocessing sentence pairs \n",
    "We've seen how to tokenize single sentences and batch them together.\n",
    "\n",
    "Now we'll focus on tasks that classify pairs of sentences.\n",
    "\n",
    "Classifying pair of sentences is a problem worth studying. \n",
    "\n",
    "Data sets with pair of sentences:\n",
    "- MRPC\n",
    "- STC-B\n",
    "- QQP\n",
    "- MNLI\n",
    "- QNLI\n",
    "- RTE\n",
    "- WNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ac05a",
   "metadata": {},
   "source": [
    "This is why models like bert are often pretrained with a dual objective: On top of the language model objective to sentence pairs, they ogten have an objetive related to sentence pairs.\n",
    "\n",
    "Example:\n",
    "\n",
    "My [mask] (name) is Bruno. I [mask] (work) with machine learning -> Sentences are in order\n",
    "\n",
    "My [mask] (name) is Bruno. This movie [mask] (is) great. -> Sentences are not in order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2a625",
   "metadata": {},
   "source": [
    "## Tokenizer to deal with pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdb677b0-e342-4a8f-9a94-20f4c4a36a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2026, 2171, 2003, 10391, 1012, 102, 1045, 2147, 2007, 3698, 4083, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer(\"My name is Bruno.\",\"I work with machine learning.\") # You pass as many sentences as you need \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1d669",
   "metadata": {},
   "source": [
    "### Having several pairs of sentences\n",
    "\n",
    "We can tokenize together by passing the list of first sentences, then the list of second sentences and remember padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f313bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\n",
    "    [\"My name is bruno\",\"Going to hear some music\"],\n",
    "    [\"I work with machine learning\",\"This place has low temperature\"],\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c898c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunojaime/Documents/Machine_learning/mlvirtualenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b002282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.5183127 , -0.68155235],\n",
       "       [-0.42416868, -0.38345772]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31aebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
