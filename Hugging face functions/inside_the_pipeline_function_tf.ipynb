{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stages of a pipeline\n",
    "1) Tokenizer -> Tokenize from raw text to tokenx\n",
    "2) Model -> The generated tokens go into the model and generate the logits\n",
    "3) Post processing -> Transforms the logits in labels and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Tokenizer\n",
    "- Text gets split into tokens\n",
    "- The tokenizer will add some special tokens, if the model expects them. Example: CLS and SEP to indicate the begining and the end of a sentence\n",
    "- The tokenizer matches each token to the unique related id of the previous trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) #from_pretrained downloads the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1996,  2326,  2001,  9788,  1998,  1996,  2833, 12090,\n",
      "          102,     0,     0,     0,     0,     0,     0],\n",
      "       [  101,  1045,  2180, 29658,  2102,  4965,  1999,  2023,  2173,\n",
      "         1010,  2027,  2024,  2061, 12726,   999,   102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs=[\n",
    "    \"The service was incredible and the food delicious\",\n",
    "    \"I wonÂ´t buy in this place, they are so rude!\"\n",
    "]\n",
    "inputs = tokenizer(raw_inputs,padding=True,truncation=True,return_tensors=\"tf\") \n",
    "# padding = True. Since we are passing two different sentences we will need to pad the shortest one in order to build an array\n",
    "# truncation = True -> We ensure that any sentence longer than the maximum the model can handle is truncated\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# downloads and stores the configuration of the model as well as the pretrained weights\n",
    "# The AutoModel class loads a model without its pretrained head\n",
    "# It will generate a high dimensional tensor which it's a representation of the sentences\n",
    "model = TFAutoModel.from_pretrained(checkpoint) \n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
