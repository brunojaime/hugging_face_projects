{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic padding\n",
    "To be able to group inputs of different lengths in the same batch, we need to add padding to the shortest ones, to be equal to the longedt text length.\n",
    "\n",
    "When dealing with a whole dataset, there are various padding strategies we can apply.\n",
    "## Strategies\n",
    "### 1. Pad all the elements of the dataset to the same length: The length of the longest sample.\n",
    "**Down sides:**\n",
    "- Bathes composed from short sentences will have a lot of padding tokens, which introduce more computation in the model that it's actually not needed\n",
    "\n",
    "### 2. Pad the elements when we batch them together, to the longest sentence inside the batch (dynamic padding)\n",
    "- Batches composed of short inputs will be smaeller than the batch containing the longest sentence in the dataset.\n",
    "**Down sides**\n",
    "All batches have different shapes, which slows down training on othe accelerators liker TPUs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers evaluate datasets accelerate -q\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunojaime/Documents/Machine_learning/mlvirtualenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset('glue','mrpc')\n",
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:00<00:00, 8331.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence1'],examples['sentence2'],padding=\"max_length\",truncation=True, max_length=128 #Here we are applying max_length padding strategys\n",
    "    )\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function,batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['idx','sentence1','sentence2'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label','labels')\n",
    "tokenized_dataset = tokenized_dataset.with_format('tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:00<00:00, 10504.45 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 6528.02 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 8871.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence1'],examples['sentence2'],truncation=True #Here we are applying batch padding strategy\n",
    "    )\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function,batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['idx','sentence1','sentence2'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label','labels')\n",
    "tokenized_dataset = tokenized_dataset.with_format('tensorflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic batching will almost always be faster on CPUs and GPUs, so we should apply when we can. If running on TPU, or neew batches fixed shapes we should use max_length strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
